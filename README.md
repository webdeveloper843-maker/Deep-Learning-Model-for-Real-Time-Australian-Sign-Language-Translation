# Real-Time Auslan Sign Language Translator  

A deep learningâ€“based prototype that captures live video, recognizes **Australian Sign Language (Auslan) signs**, and translates them into English text **in real time**.  

This system demonstrates **>90% accuracy** and **<300 ms latency** on commodity hardware, making it a practical, deployable solution for accessibility and inclusive communication.  

---

## âœ¨ Features
- ðŸ“· **Live camera feed integration** with real-time recognition  
- âœ‹ **Hand detection & segmentation** using preprocessing pipeline  
- ðŸ§  **3D-CNN + BiLSTM + Attention model** for spatiotemporal recognition  
- âš¡ **Optimized inference** with TensorFlow Lite / ONNX Runtime  
- ðŸ“Š **Performance:** 91â€“93% Top-1 accuracy, 96% Top-5 accuracy  
- ðŸ–¥ï¸ **Prototype UI** with live output and confidence score display  
- ðŸ”’ **Privacy by design** â€“ local-only processing, no retention without consent  

---

## ðŸ“Œ Project Overview
The project aims to provide a **real-time Auslan sign translator** that can be used in **customer service kiosks, educational platforms, and digital applications**.  

It addresses the lack of inclusive tools for the Deaf and hard-of-hearing community by combining **state-of-the-art AI models**, **accessible design**, and **ethical ICT practices**.  

---

## ðŸ“‚ Project Structure



---

## âš™ï¸ Installation

1. **Clone the repository:**
```bash
git clone https://github.com/your-username/sign-language-translator.git
cd sign-language-translator
python -m venv venv
source venv/bin/activate    # Linux/Mac
venv\Scripts\activate       # Windows
pip install -r requirements.txt
python models/train.py --config configs/config.yaml
python inference/run_realtime.py
python ui/app.py

